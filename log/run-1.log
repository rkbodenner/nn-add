% Hm. The regularization, it does nothing.
% Maybe add more hidden nodes? Still does poorly after 50 training iterations.

Architecture: 2 -> 3 -> 99
Initial weights
Theta1
-0.019380
-0.107403
0.005286
0.119460
-0.025707
0.015675
-0.019970
0.100863
-0.030072

Theta2
-0.070688
-0.090001
-0.029331
-0.053548
-0.001383
0.076021
-0.105896
-0.007291
0.053326
-0.028537
-0.051243
0.068672
0.073785
0.087110
0.010247
0.098931
-0.092912
0.059916
0.038636
0.016294
0.115709
-0.005892
0.019460
0.079807
-0.006018
0.002918
-0.113422
0.088006
0.045410
0.007141
0.111219
0.012115
-0.028249
-0.093520
0.075111
-0.003119
0.008740
0.101661
-0.032272
-0.078635
-0.099593
0.001703
-0.104317
0.069837
0.029423
-0.046938
0.095632
-0.040770
0.062532
0.026098
-0.051228
0.016375
-0.049916
0.045484
-0.016321
0.005600
0.054313
-0.024638
0.093440
-0.040672
0.039855
-0.034581
-0.116445
-0.052208
0.035926
-0.077585
0.110545
-0.067844
-0.096889
-0.098358
0.107047
0.081860
0.116162
0.093505
0.009911
-0.010078
-0.116940
-0.045941
-0.086060
-0.058450
-0.066236
-0.091556
0.035482
-0.067655
-0.064341
-0.076456
-0.059912
0.112314
0.047654
0.014374
-0.013524
0.058280
-0.074952
-0.026798
-0.038388
0.019956
-0.065680
0.075255
0.005279
-0.092925
0.014691
-0.035491
0.000548
-0.035675
-0.043197
-0.037177
-0.082893
-0.079255
0.010864
-0.078168
-0.064384
0.048906
0.049862
0.103532
-0.084732
0.044019
0.050945
-0.030987
-0.024442
0.112330
-0.050325
-0.055903
-0.075225
0.072916
-0.102639
-0.117856
-0.038792
-0.076611
0.075242
-0.095805
-0.095130
0.071759
-0.037729
0.118786
0.080019
-0.020598
-0.074344
-0.087067
0.027467
0.062112
0.092421
0.037991
-0.071799
0.036439
-0.012149
-0.066330
0.091981
0.108759
0.057905
-0.020416
0.008124
0.021078
0.090996
0.093159
0.055830
-0.100374
-0.080442
-0.031711
0.094120
0.000114
-0.044463
0.110254
0.081977
0.044083
-0.067685
-0.102068
-0.089957
0.028576
0.050218
0.073339
-0.096858
-0.052668
-0.001460
-0.031014
-0.097723
0.085299
-0.009771
-0.033650
0.009262
-0.027856
0.059538
0.048597
0.070501
-0.004319
-0.009246
0.109351
-0.025552
-0.091438
-0.116411
0.094877
-0.069099
0.111436
0.013031
-0.033849
0.035720
-0.039158
-0.118641
0.085335
0.085354
0.080386
-0.077876
-0.077728
-0.062702
0.084215
0.027718
0.078737
-0.016623
-0.118375
-0.102070
-0.036921
0.015726
0.060155
0.062209
0.054188
0.059583
-0.037961
-0.093225
-0.066979
0.019030
0.105690
0.040852
-0.033613
0.082631
-0.042385
0.086895
-0.036945
0.023886
-0.030390
0.088107
0.116376
-0.108767
0.057273
0.075611
-0.071354
0.044879
-0.016015
0.094207
0.099707
-0.105780
-0.051654
-0.047729
0.048735
-0.102830
-0.072252
0.039028
0.040902
0.066071
-0.097356
0.119784
-0.021232
-0.079652
0.107615
-0.021348
-0.113231
-0.052460
-0.053391
-0.101496
0.063546
-0.038370
0.061572
-0.042256
-0.096633
-0.107864
-0.028990
0.037954
-0.048937
0.028212
-0.113183
0.085248
-0.067950
-0.018333
0.043960
0.002654
0.028766
-0.051501
0.026998
-0.044239
0.085427
0.087519
0.069465
0.022639
0.056198
-0.101430
0.011165
0.010701
-0.096549
0.031038
0.070622
-0.049471
0.068870
0.087170
0.109599
-0.016781
0.107565
0.055988
-0.010164
0.029537
0.075680
-0.067056
0.060805
0.117821
-0.070139
0.014665
-0.009048
0.029184
0.045485
0.065198
-0.044060
0.098654
-0.065935
0.024771
0.052942
-0.111731
0.051978
0.070257
-0.010321
-0.107046
0.051387
0.022346
0.060256
0.036748
-0.021228
0.017768
0.028588
0.082365
0.029044
-0.056340
-0.034969
0.062881
-0.059994
0.068480
-0.007781
0.103152
-0.061036
0.109995
-0.038841
0.043322
-0.029796
0.037401
-0.080860
-0.111020
-0.028126
0.014348
-0.027282
-0.061687
-0.110289
-0.089845
0.069105
-0.019848
-0.004867
-0.074770
-0.069085
-0.045589
-0.093108
0.022439
0.063064
-0.111981
0.075215
0.015942
-0.108373
-0.089696
-0.077108
0.024877
0.004490
-0.018122
-0.021548
-0.014550
-0.028858
-0.040932
0.111514
-0.044311
0.000667
-0.053859
-0.039602
-0.056810
0.001249
0.114786
0.063129
0.112605
0.013481
0.111884
-0.065278
0.062100
-0.012075
-0.059129
0.039435
-0.070333
-0.105863
-0.075428
0.013792
-0.066717
0.096514
-0.113707
0.043370
-0.088882
-0.026229

Lambda: 1.000000
Initial cost: 68.572091
Iteration     1 | Cost: 2.368107e+01
Iteration    50 | Cost: 5.122879e+00
Training complete. Final cost: 23.681073
Training complete. Final cost: 7.060918
Training complete. Final cost: 6.965979
Training complete. Final cost: 6.788049
Training complete. Final cost: 5.924169
Training complete. Final cost: 5.758850
Training complete. Final cost: 5.673293
Training complete. Final cost: 5.632326
Training complete. Final cost: 5.608910
Training complete. Final cost: 5.590135
Training complete. Final cost: 5.548944
Training complete. Final cost: 5.532343
Training complete. Final cost: 5.508848
Training complete. Final cost: 5.496460
Training complete. Final cost: 5.479728
Training complete. Final cost: 5.450930
Training complete. Final cost: 5.425870
Training complete. Final cost: 5.415844
Training complete. Final cost: 5.412384
Training complete. Final cost: 5.402048
Training complete. Final cost: 5.399901
Training complete. Final cost: 5.388517
Training complete. Final cost: 5.383509
Training complete. Final cost: 5.379224
Training complete. Final cost: 5.376899
Training complete. Final cost: 5.372674
Training complete. Final cost: 5.358740
Training complete. Final cost: 5.358156
Training complete. Final cost: 5.342076
Training complete. Final cost: 5.341894
Training complete. Final cost: 5.341882
Training complete. Final cost: 5.341416
Training complete. Final cost: 5.341332
Training complete. Final cost: 5.341126
Training complete. Final cost: 5.340332
Training complete. Final cost: 5.337734
Training complete. Final cost: 5.319992
Training complete. Final cost: 5.291146
Training complete. Final cost: 5.285350
Training complete. Final cost: 5.278778
Training complete. Final cost: 5.232888
Training complete. Final cost: 5.218103
Training complete. Final cost: 5.203595
Training complete. Final cost: 5.196622
Training complete. Final cost: 5.187691
Training complete. Final cost: 5.154973
Training complete. Final cost: 5.145539
Training complete. Final cost: 5.130761
Training complete. Final cost: 5.125949
Training complete. Final cost: 5.122879
octave:16> predict(final_Theta1, final_Theta2, [1 1])
ans =  22
octave:17> predict(final_Theta1, final_Theta2, [1 2])
ans =  22
octave:18> predict(final_Theta1, final_Theta2, [1 3])
ans =  22
octave:19> predict(final_Theta1, final_Theta2, [1 4])
ans =  22
octave:20> predict(final_Theta1, final_Theta2, [1 5])
ans =  22
octave:21> predict(final_Theta1, final_Theta2, [1 6])
ans =  22
octave:22> predict(final_Theta1, final_Theta2, [1 7])
ans =  22
octave:23> predict(final_Theta1, final_Theta2, [1 8])
ans =  22
octave:24> predict(final_Theta1, final_Theta2, [1 9])
ans =  22
octave:25> predict(final_Theta1, final_Theta2, [1 10])
ans =  22
octave:26> predict(final_Theta1, final_Theta2, [1 11])
ans =  22
octave:27> predict(final_Theta1, final_Theta2, [1 12])
ans =  22
octave:28> predict(final_Theta1, final_Theta2, [1 13])
ans =  22
octave:29> predict(final_Theta1, final_Theta2, [1 14])
ans =  22
octave:30> predict(final_Theta1, final_Theta2, [1 15])
ans =  22
octave:31> predict(final_Theta1, final_Theta2, [1 16])
ans =  22
octave:32> predict(final_Theta1, final_Theta2, [1 17])
ans =  22
octave:33> predict(final_Theta1, final_Theta2, [1 18])
ans =  22
octave:34> predict(final_Theta1, final_Theta2, [1 28])
ans =  22
octave:35> predict(final_Theta1, final_Theta2, [1 38])
ans =  36
octave:36> predict(final_Theta1, final_Theta2, [1 48])
ans =  40
octave:37> predict(final_Theta1, final_Theta2, [1 58])
ans =  66
octave:38> predict(final_Theta1, final_Theta2, [1 68])
ans =  66
